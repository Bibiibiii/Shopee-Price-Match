{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL EDA & Naive Model Ashley.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj8F0XxSB9mt"
      },
      "source": [
        "import plotly.express as px\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "# import plotly.express as px\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "#Text Color\n",
        "from termcolor import colored\n",
        "\n",
        "#Data Preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "#NLP\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#WordCloud\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "#Text Processing\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('popular')\n",
        "\n",
        "#Language Detection\n",
        "!pip install langdetect\n",
        "import langdetect\n",
        "\n",
        "#Sentiment\n",
        "from textblob import TextBlob\n",
        "\n",
        "#ner\n",
        "import spacy\n",
        "\n",
        "#Vectorizer\n",
        "from sklearn import feature_extraction, manifold\n",
        "\n",
        "#Word Embedding\n",
        "import gensim.downloader as gensim_api\n",
        "\n",
        "#Topic Modeling\n",
        "import gensim\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-keTzi3UZu9u"
      },
      "source": [
        "# Basic\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "from tqdm.autonotebook import tqdm\n",
        "import string\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "# NLP\n",
        "\n",
        "import spacy\n",
        "#!pip install -U spacy\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw9njf2VcbJ_"
      },
      "source": [
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1D3DgeYfo_L"
      },
      "source": [
        "#!python -m spacy download en_core_web_sm\n",
        "\n",
        "#!python -m spacy link en_core_web_sm en\n",
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz --no-deps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGcGd_iACHtq"
      },
      "source": [
        "!pip install hvplot\n",
        "import hvplot.pandas  # custom install\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "#from bq_helper import BigQueryHelper\n",
        "#from dask import bag, diagnostics \n",
        "from urllib import request\n",
        "\n",
        "import missingno as msno"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDEbUaE-CeqU"
      },
      "source": [
        "#!pip install -e git+https://github.com/SohierDane/BigQuery_Helper#egg=bq_helper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGQPAjVyCn_n"
      },
      "source": [
        "#!pip install bq_helper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmplkDApCgj3"
      },
      "source": [
        "# Import train test label data\n",
        "train = pd.read_csv('/content/drive/Shareddrives/deep learning project/data/shopee-product-matching/train.csv')\n",
        "test = pd.read_csv('/content/drive/Shareddrives/deep learning project/data/shopee-product-matching/test.csv')\n",
        "sample = pd.read_csv('/content/drive/Shareddrives/deep learning project/data/shopee-product-matching/sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVxMCG7FAFg6"
      },
      "source": [
        "# Import the image data\n",
        "\n",
        "train_jpg_directory = '/content/drive/Shareddrives/deep learning project/data/shopee-product-matching/train_images'\n",
        "test_jpg_directory = '/content/drive/Shareddrives/deep learning project/data/shopee-product-matching/test_images'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxeKyRO5GFFJ"
      },
      "source": [
        "def getImagePaths(path):\n",
        "    \"\"\"\n",
        "    Function to Combine Directory Path with individual Image Paths\n",
        "    \n",
        "    parameters: path(string) - Path of directory\n",
        "    returns: image_names(string) - Full Image Path\n",
        "    \"\"\"\n",
        "    image_names = []\n",
        "    for dirname, _, filenames in os.walk(path):\n",
        "        for filename in filenames:\n",
        "            fullpath = os.path.join(dirname, filename)\n",
        "            image_names.append(fullpath)\n",
        "    return image_names\n",
        "\n",
        "#Get complete image paths for train and test datasets\n",
        "train_images_path = getImagePaths(train_jpg_directory)\n",
        "test_images_path = getImagePaths(test_jpg_directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWzK9eKKH1HI"
      },
      "source": [
        "print(f\"Number of train images: {colored(len(train_images_path), 'yellow')}\")\n",
        "print(f\"Number of test images:  {colored(len(test_images_path), 'yellow')}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4PyVrBmGe50"
      },
      "source": [
        "def display_multiple_img(images_paths, rows, cols):\n",
        "    \"\"\"\n",
        "    Function to Display Images from Dataset.\n",
        "    \n",
        "    parameters: images_path(string) - Paths of Images to be displayed\n",
        "                rows(int) - No. of Rows in Output\n",
        "                cols(int) - No. of Columns in Output\n",
        "    \"\"\"\n",
        "    figure, ax = plt.subplots(nrows=rows,ncols=cols,figsize=(16,8) ) #create a canvas\n",
        "    for ind,image_path in enumerate(images_paths):\n",
        "        image=cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
        "        try:\n",
        "            ax.ravel()[ind].imshow(image)\n",
        "            ax.ravel()[ind].set_axis_off()\n",
        "        except:\n",
        "            continue;\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQu6Pe26HFSM"
      },
      "source": [
        "display_multiple_img(train_images_path[100:130], 6,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE5DxWCpHGLZ"
      },
      "source": [
        "# Image title\n",
        "stopwords = set(STOPWORDS) \n",
        "wordcloud = WordCloud(width = 800, \n",
        "                      height = 800,\n",
        "                      background_color ='white',\n",
        "                      min_font_size = 10,\n",
        "                      stopwords = stopwords,).generate(' '.join(train['title'])) \n",
        "\n",
        "# plot the WordCloud image                        \n",
        "plt.figure(figsize = (8, 8), facecolor = None) \n",
        "plt.imshow(wordcloud) \n",
        "plt.axis(\"off\") \n",
        "plt.tight_layout(pad = 0) \n",
        "\n",
        "plt.show() \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcaZZZqkOA1n"
      },
      "source": [
        "# Remove stopwords and clean our labels/titles\n",
        "def preprocess_text(text, flg_stemm=False, flg_lemm=True):\n",
        "\n",
        "    lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "    \n",
        "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
        "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
        "            \n",
        "    ## Tokenize (convert from string to list)\n",
        "    lst_text = text.split()\n",
        "    ## remove Stopwords\n",
        "    if lst_stopwords is not None:\n",
        "        lst_text = [word for word in lst_text if word not in \n",
        "                    lst_stopwords]\n",
        "                \n",
        "    ## Stemming (remove -ing, -ly, ...)\n",
        "    if flg_stemm == True:\n",
        "        ps = nltk.stem.porter.PorterStemmer()\n",
        "        lst_text = [ps.stem(word) for word in lst_text]\n",
        "                \n",
        "    ## Lemmatisation (convert the word into root word)\n",
        "    if flg_lemm == True:\n",
        "        lem = nltk.stem.wordnet.WordNetLemmatizer()    \n",
        "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
        "            \n",
        "    ## back to string from list\n",
        "    text = \" \".join(lst_text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THHeH-GaOIv7"
      },
      "source": [
        "#Clean Address\n",
        "train[\"clean_title\"] = train[\"title\"].apply(lambda x: preprocess_text(x, flg_stemm=False, flg_lemm=True, ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYz4C40QORbR"
      },
      "source": [
        "#Length of Title\n",
        "train['clean_title_len'] = train['clean_title'].apply(lambda x: len(x))\n",
        "\n",
        "#Word Count\n",
        "train['clean_title_word_count'] =train[\"clean_title\"].apply(lambda x: len(str(x).split(\" \")))\n",
        "\n",
        "#Character Count\n",
        "train['clean_title_char_count'] = train[\"clean_title\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\n",
        "\n",
        "#Average Word Length\n",
        "train['clean_title_avg_word_length'] = train['clean_title_char_count'] / train['clean_title_word_count']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aiHMb7fORd6"
      },
      "source": [
        "def plot_distribution(x, title):\n",
        "\n",
        "    fig = px.histogram(\n",
        "    train, \n",
        "    x = x,\n",
        "    width = 800,\n",
        "    height = 500,\n",
        "    title = title\n",
        "    )\n",
        "    \n",
        "    fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTKP3KPsORgW"
      },
      "source": [
        "plot_distribution(x = 'clean_title_len', title = 'Title Length Distribution')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRyG2rxZORiw"
      },
      "source": [
        "plot_distribution(x = 'clean_title_word_count', title = 'Word Count Distribution')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C38TKoybOix8"
      },
      "source": [
        "plot_distribution(x = 'clean_title_char_count', title = 'Character Count Distribution')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhGX13BCP2X_"
      },
      "source": [
        "# Image Label Groups by No. of Images\n",
        "top10_names = train['label_group'].value_counts().index.tolist()[:15]\n",
        "top10_values = train['label_group'].value_counts().tolist()[:15]\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "sns.barplot(x=top10_names, y=top10_values)\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel(\"Label Group\")\n",
        "plt.ylabel(\"Image Count\")\n",
        "plt.title(\"Top-15 Label Groups by Image Count\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM9DpAWYTNYE"
      },
      "source": [
        "!pip install 'fsspec>=0.3.3'\n",
        "from dask import bag, diagnostics "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9mEF6a-XAmx"
      },
      "source": [
        "data_dir = '/content/drive/Shareddrives/deep learning project/data/shopee-product-matching'\n",
        "\n",
        "train_file_path = os.path.join(data_dir, 'train.csv')\n",
        "test_file_path = os.path.join(data_dir, 'test.csv')\n",
        "sample_sub_file_path = os.path.join(data_dir, 'sample_submission.csv')\n",
        "train_images_path = os.path.join(data_dir, 'train_images')\n",
        "test_images_path = os.path.join(data_dir, 'test_images')\n",
        "\n",
        "train_df = pd.read_csv(train_file_path)\n",
        "test_df = pd.read_csv(test_file_path)\n",
        "sub_df = pd.read_csv(sample_sub_file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVG8RI1cRlNi"
      },
      "source": [
        "def show_image(class_num, examples=2, train_df=train_df, train_images_path=train_images_path):\n",
        "    image_list = train_df[train_df['label_group'] == class_num]['image'].sample(frac=1)[:examples].to_list()\n",
        "    plt.figure(figsize=(20,10))\n",
        "    for i, img in enumerate(image_list):\n",
        "        full_path = os.path.join(train_images_path, img)\n",
        "        img = Image.open(full_path)\n",
        "        plt.subplot(1 ,examples, i%examples +1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img)\n",
        "        plt.title(f'Class: {class_num}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHj_927gWv74"
      },
      "source": [
        "# Naive Model: \n",
        "#We assume  assume the products with the exact same title are similar products. \n",
        "def prepare_text(text, nlp=nlp):\n",
        "    '''\n",
        "    Returns the text after stop-word removal and lemmatization.\n",
        "    text - Sentence to be processed\n",
        "    nlp - Spacy NLP model\n",
        "    '''\n",
        "    doc = nlp(text)\n",
        "    lemma_list = [token.lemma_ for token in doc if not token.is_stop]\n",
        "    lemmatized_sentence = ' '.join(lemma_list)\n",
        "        \n",
        "    return lemmatized_sentence\n",
        "\n",
        "tqdm.pandas()\n",
        "test_df['title'] = test_df['title'].progress_apply(prepare_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuG-wiRgXzA0"
      },
      "source": [
        "image_list = train_df[train_df['label_group'] == 3]['image'].sample(frac=1)[:2].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zkjTxFDYd0X"
      },
      "source": [
        "train_df.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}